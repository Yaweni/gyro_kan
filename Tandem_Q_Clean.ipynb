{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8064bb",
   "metadata": {},
   "source": [
    "# Tandem-Q Architecture: Clean Implementation\n",
    "\n",
    "This notebook demonstrates the Tandem-Q recurrent neural network, a novel architecture using unitary hypercomplex recurrence for sequence modeling. It leverages quaternions to perform geometric transformations that preserve gradients and model non-commutative operations.\n",
    "\n",
    "## Key Innovations:\n",
    "- **Unitary Rotations**: Prevents gradient vanishing/explosion.\n",
    "- **Non-Commutative Algebra**: Captures sequence order.\n",
    "- **Tandem Mechanism**: Decouples rotation and scaling for optimization.\n",
    "\n",
    "We'll implement the core components and demonstrate on the Adding Problem (pass-through memory) and Sequential MNIST (convolutional adaptation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a3c6d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73e9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484760a",
   "metadata": {},
   "source": [
    "## 2. Quaternion Operations\n",
    "\n",
    "Quaternions enable 4D rotations and non-commutative operations. We implement multiplication and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03269617",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def quaternion_mul(q1, q2):\n",
    "    r1, i1, j1, k1 = q1.unbind(-1)\n",
    "    r2, i2, j2, k2 = q2.unbind(-1)\n",
    "    r = r1*r2 - i1*i2 - j1*j2 - k1*k2\n",
    "    i = r1*i2 + i1*r2 + j1*k2 - k1*j2\n",
    "    j = r1*j2 - i1*k2 + j1*r2 + k1*i2\n",
    "    k = r1*k2 + i1*j2 - j1*i2 + k1*r2\n",
    "    return torch.stack((r, i, j, k), dim=-1)\n",
    "\n",
    "@torch.jit.script\n",
    "def quaternion_norm(q):\n",
    "    return torch.norm(q, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f9c72",
   "metadata": {},
   "source": [
    "## 3. Tandem-Q Cell\n",
    "\n",
    "The core cell that performs unitary rotations and controlled scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8555a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TandemQCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih = nn.Linear(input_size, hidden_size * 5)\n",
    "        self.hh = nn.Linear(hidden_size * 4, hidden_size * 5)\n",
    "        \n",
    "        # Initialize for stability\n",
    "        nn.init.normal_(self.ih.weight, 0, 0.01)\n",
    "        nn.init.zeros_(self.ih.bias)\n",
    "        nn.init.zeros_(self.hh.weight)\n",
    "        nn.init.zeros_(self.hh.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq, _ = x.size()\n",
    "        \n",
    "        gates_in = self.ih(x)\n",
    "        h_r = torch.ones(b, self.hidden_size, device=x.device)\n",
    "        h_i = torch.zeros(b, self.hidden_size, device=x.device)\n",
    "        h_j = torch.zeros(b, self.hidden_size, device=x.device)\n",
    "        h_k = torch.zeros(b, self.hidden_size, device=x.device)\n",
    "        \n",
    "        for t in range(seq):\n",
    "            g_in = gates_in[:, t, :]\n",
    "            h_cat = torch.cat((h_r, h_i, h_j, h_k), dim=1)\n",
    "            g_hh = self.hh(h_cat)\n",
    "            raw = g_in + g_hh\n",
    "            raw = raw.view(b, self.hidden_size, 5)\n",
    "            \n",
    "            rot_raw = raw[:, :, :4]\n",
    "            rot_r = rot_raw[:, :, 0] + 1.0\n",
    "            rot_i, rot_j, rot_k = rot_raw[:, :, 1], rot_raw[:, :, 2], rot_raw[:, :, 3]\n",
    "            \n",
    "            scale = torch.sigmoid(raw[:, :, 4] + 3.0)\n",
    "            norm = quaternion_norm(torch.stack((rot_r, rot_i, rot_j, rot_k), dim=-1))\n",
    "            norm_squeezed = norm.squeeze(-1)\n",
    "            \n",
    "            new_r, new_i, new_j, new_k = quaternion_mul(\n",
    "                torch.stack((rot_r/norm_squeezed, rot_i/norm_squeezed, rot_j/norm_squeezed, rot_k/norm_squeezed), dim=-1),\n",
    "                torch.stack((h_r, h_i, h_j, h_k), dim=-1)\n",
    "            ).unbind(-1)\n",
    "            \n",
    "            h_r, h_i, h_j, h_k = new_r * scale, new_i * scale, new_j * scale, new_k * scale\n",
    "            \n",
    "        return torch.stack((h_r, h_i, h_j, h_k), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd168e",
   "metadata": {},
   "source": [
    "## 4. Adapt for Pass-Through Memory: Adding Problem\n",
    "\n",
    "The Adding Problem requires remembering two numbers separated by noise. Tandem-Q's unitary rotations preserve information over long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be66753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddingProblemDataset(Dataset):\n",
    "    def __init__(self, seq_len=50, size=1000):\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self, idx):\n",
    "        values = np.random.uniform(0, 1, (self.seq_len, 1)).astype(np.float32)\n",
    "        mask = np.zeros((self.seq_len, 1), dtype=np.float32)\n",
    "        positions = np.random.choice(self.seq_len, size=2, replace=False)\n",
    "        mask[positions] = 1.0\n",
    "        inputs = np.concatenate((values, mask), axis=1)\n",
    "        target = np.sum(values[positions])\n",
    "        return torch.tensor(inputs), torch.tensor(target)\n",
    "\n",
    "class AddingModel(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.cell = TandemQCell(2, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 4, 1)\n",
    "    def forward(self, x):\n",
    "        h = self.cell(x)\n",
    "        return self.fc(h.view(x.size(0), -1)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db687f66",
   "metadata": {},
   "source": [
    "## 5. Adapt for Convolution: Sequential MNIST\n",
    "\n",
    "For image-like sequences, we process row-by-row. The Tandem-Q cell handles the sequential nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe75aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.cell = TandemQCell(28, hidden_dim)  # 28 pixels per row\n",
    "        self.fc = nn.Linear(hidden_dim * 4, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)  # (B, 28, 28)\n",
    "        h = self.cell(x)\n",
    "        return self.fc(h.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2eb56",
   "metadata": {},
   "source": [
    "## 6. Training Setup\n",
    "\n",
    "We set up the models, optimizers, and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed0031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Problem\n",
    "adding_model = AddingModel(hidden_dim=16).to(device)\n",
    "adding_optimizer = optim.Adam(adding_model.parameters(), lr=0.005)\n",
    "adding_criterion = nn.MSELoss()\n",
    "\n",
    "adding_train_ds = AddingProblemDataset(seq_len=50, size=5000)\n",
    "adding_train_loader = DataLoader(adding_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "mnist_train_ds = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "mnist_train_loader = DataLoader(mnist_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "mnist_model = MNISTModel(hidden_dim=32).to(device)\n",
    "mnist_optimizer = optim.Adam(mnist_model.parameters(), lr=0.002)\n",
    "mnist_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae9f47",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Train on the Adding Problem and Sequential MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d1eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Adding Problem...\n",
      "Epoch 1 MSE: 0.1418\n",
      "Epoch 2 MSE: 0.0454\n",
      "Epoch 3 MSE: 0.0190\n",
      "Epoch 4 MSE: 0.0102\n",
      "Epoch 5 MSE: 0.0101\n",
      "Epoch 6 MSE: 0.0081\n",
      "Epoch 7 MSE: 0.0058\n",
      "Epoch 8 MSE: 0.0055\n",
      "Epoch 9 MSE: 0.0067\n",
      "Epoch 10 MSE: 0.0061\n",
      "Epoch 11 MSE: 0.0063\n",
      "\n",
      "Training Sequential MNIST...\n",
      "Epoch 1 Acc: 95.58%\n",
      "Epoch 2 Acc: 97.41%\n",
      "Epoch 3 Acc: 97.91%\n",
      "Epoch 4 Acc: 98.15%\n",
      "Epoch 5 Acc: 98.40%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m12\u001b[39m):\n\u001b[32m     19\u001b[39m     correct = \u001b[32m0\u001b[39m; total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmnist_train_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmnist_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:917\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    913\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    914\u001b[39m     )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m dtype = tensor.dtype\n\u001b[32m    920\u001b[39m mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train Adding\n",
    "print(\"Training Adding Problem...\")\n",
    "for epoch in range(1, 12):\n",
    "    total_loss = 0\n",
    "    for data, target in adding_train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        adding_optimizer.zero_grad()\n",
    "        output = adding_model(data)\n",
    "        loss = adding_criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(adding_model.parameters(), 1.0)\n",
    "        adding_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} MSE: {total_loss / len(adding_train_loader):.4f}\")\n",
    "\n",
    "# Train MNIST\n",
    "print(\"\\nTraining Sequential MNIST...\")\n",
    "for epoch in range(1, 12):\n",
    "    correct = 0; total = 0\n",
    "    for x, y in mnist_train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        mnist_optimizer.zero_grad()\n",
    "        out = mnist_model(x)\n",
    "        loss = mnist_criterion(out, y)\n",
    "        loss.backward()\n",
    "        mnist_optimizer.step()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    print(f\"Epoch {epoch} Acc: {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc3767",
   "metadata": {},
   "source": [
    "## 8. Evaluate Performance\n",
    "\n",
    "Test the models on held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504fa760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Test MSE: 0.0119\n",
      "MNIST Test Acc: 97.93%\n",
      "\n",
      "Tandem-Q demonstrates strong performance on memory and sequential tasks with efficient parameter usage!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Adding\n",
    "adding_test_ds = AddingProblemDataset(seq_len=50, size=1000)\n",
    "adding_test_loader = DataLoader(adding_test_ds, batch_size=64)\n",
    "adding_model.eval()\n",
    "total_mse = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in adding_test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = adding_model(data)\n",
    "        total_mse += adding_criterion(output, target).item()\n",
    "print(f\"Adding Test MSE: {total_mse / len(adding_test_loader):.4f}\")\n",
    "\n",
    "# Evaluate MNIST\n",
    "mnist_test_ds = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "mnist_test_loader = DataLoader(mnist_test_ds, batch_size=64)\n",
    "mnist_model.eval()\n",
    "correct = 0; total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in mnist_test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = mnist_model(x)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "print(f\"MNIST Test Acc: {100*correct/total:.2f}%\")\n",
    "\n",
    "print(\"\\nTandem-Q demonstrates strong performance on memory and sequential tasks with efficient parameter usage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4246de",
   "metadata": {},
   "source": [
    "## Additional Experiments\n",
    "\n",
    "Here we demonstrate adaptations of the Tandem-Q architecture for various sequence tasks, showing its versatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558f84d",
   "metadata": {},
   "source": [
    "### Copy Problem: Delayed Memory Recall\n",
    "\n",
    "The model must remember a sequence and reproduce it after a delay, using embeddings and sequence output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6e586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureRotationCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ih = nn.Linear(input_size, hidden_size * 4)\n",
    "    def forward(self, x, h):\n",
    "        b, seq, _ = x.size()\n",
    "        rot_command = self.ih(x).view(b, self.hidden_size, 4)\n",
    "        rot_command = rot_command / (quaternion_norm(rot_command) + 1e-8)\n",
    "        return quaternion_mul(rot_command, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b57a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy Epoch 1 Loss: 2.0821 | Accuracy: 7.82%\n",
      "Copy Epoch 2 Loss: 1.7637 | Accuracy: 5.38%\n",
      "Copy Epoch 3 Loss: 1.2856 | Accuracy: 6.97%\n",
      "Copy Epoch 4 Loss: 0.9454 | Accuracy: 10.92%\n",
      "Copy Epoch 5 Loss: 0.8290 | Accuracy: 11.50%\n",
      "Copy Epoch 6 Loss: 0.7819 | Accuracy: 12.35%\n",
      "Copy Epoch 7 Loss: 0.7567 | Accuracy: 13.62%\n",
      "Copy Epoch 8 Loss: 0.7413 | Accuracy: 14.65%\n",
      "Copy Epoch 9 Loss: 0.7286 | Accuracy: 16.09%\n",
      "Copy Epoch 10 Loss: 0.7157 | Accuracy: 18.60%\n",
      "Copy Epoch 11 Loss: 0.7025 | Accuracy: 21.44%\n",
      "Copy Epoch 12 Loss: 0.6914 | Accuracy: 22.98%\n",
      "Copy Epoch 13 Loss: 0.6832 | Accuracy: 23.18%\n",
      "Copy Epoch 14 Loss: 0.6761 | Accuracy: 24.21%\n",
      "Copy Epoch 15 Loss: 0.6700 | Accuracy: 24.65%\n",
      "Copy Epoch 16 Loss: 0.6589 | Accuracy: 26.93%\n",
      "Copy Epoch 17 Loss: 0.6465 | Accuracy: 28.83%\n",
      "Copy Epoch 18 Loss: 0.6384 | Accuracy: 29.54%\n",
      "Copy Epoch 19 Loss: 0.6314 | Accuracy: 30.24%\n",
      "Copy Epoch 20 Loss: 0.6281 | Accuracy: 30.35%\n",
      "Copy Epoch 21 Loss: 0.6224 | Accuracy: 31.19%\n",
      "Copy Epoch 22 Loss: 0.6199 | Accuracy: 30.80%\n",
      "Copy Epoch 23 Loss: 0.6148 | Accuracy: 31.86%\n",
      "Copy Epoch 24 Loss: 0.6118 | Accuracy: 32.06%\n",
      "Copy Epoch 25 Loss: 0.6058 | Accuracy: 33.22%\n",
      "Copy Epoch 26 Loss: 0.6016 | Accuracy: 34.08%\n",
      "Copy Epoch 27 Loss: 0.5929 | Accuracy: 35.19%\n",
      "Copy Epoch 28 Loss: 0.5875 | Accuracy: 35.45%\n",
      "Copy Epoch 29 Loss: 0.5809 | Accuracy: 36.49%\n",
      "Copy Epoch 30 Loss: 0.5743 | Accuracy: 37.67%\n",
      "Copy Epoch 31 Loss: 0.5695 | Accuracy: 37.88%\n",
      "Copy Epoch 32 Loss: 0.5646 | Accuracy: 38.07%\n",
      "Copy Epoch 33 Loss: 0.5618 | Accuracy: 38.19%\n",
      "Copy Epoch 34 Loss: 0.5557 | Accuracy: 39.05%\n",
      "Copy Epoch 35 Loss: 0.5526 | Accuracy: 39.66%\n",
      "Copy Epoch 36 Loss: 0.5509 | Accuracy: 39.22%\n",
      "Copy Epoch 37 Loss: 0.5481 | Accuracy: 40.01%\n",
      "Copy Epoch 38 Loss: 0.5443 | Accuracy: 39.83%\n",
      "Copy Epoch 39 Loss: 0.5397 | Accuracy: 40.39%\n",
      "Copy Epoch 40 Loss: 0.5400 | Accuracy: 40.32%\n",
      "Copy Epoch 41 Loss: 0.5381 | Accuracy: 40.45%\n",
      "Copy Epoch 42 Loss: 0.5335 | Accuracy: 40.51%\n",
      "Copy Epoch 43 Loss: 0.5340 | Accuracy: 40.45%\n",
      "Copy Epoch 44 Loss: 0.5290 | Accuracy: 41.39%\n",
      "Copy Epoch 45 Loss: 0.5147 | Accuracy: 42.94%\n",
      "Copy Epoch 46 Loss: 0.5071 | Accuracy: 44.58%\n",
      "Copy Epoch 47 Loss: 0.5026 | Accuracy: 44.68%\n",
      "Copy Epoch 48 Loss: 0.4960 | Accuracy: 45.32%\n",
      "Copy Epoch 49 Loss: 0.4910 | Accuracy: 46.15%\n",
      "Copy Epoch 50 Loss: 0.4898 | Accuracy: 46.11%\n",
      "Copy Epoch 51 Loss: 0.4826 | Accuracy: 46.80%\n",
      "Copy Epoch 52 Loss: 0.4832 | Accuracy: 46.86%\n",
      "Copy Epoch 53 Loss: 0.4803 | Accuracy: 46.54%\n",
      "Copy Epoch 54 Loss: 0.4749 | Accuracy: 47.29%\n",
      "Copy Epoch 55 Loss: 0.4731 | Accuracy: 47.45%\n",
      "Copy Epoch 56 Loss: 0.4735 | Accuracy: 47.22%\n",
      "Copy Epoch 57 Loss: 0.4695 | Accuracy: 47.84%\n",
      "Copy Epoch 58 Loss: 0.4666 | Accuracy: 47.63%\n",
      "Copy Epoch 59 Loss: 0.4632 | Accuracy: 48.14%\n"
     ]
    }
   ],
   "source": [
    "class CopyTaskDataset(Dataset):\n",
    "    def __init__(self, seq_len=10, delay=20, size=2000, num_classes=8):\n",
    "        self.seq_len = seq_len\n",
    "        self.delay = delay\n",
    "        self.size = size\n",
    "        self.num_classes = num_classes\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = np.random.randint(1, self.num_classes + 1, size=self.seq_len)\n",
    "        zeros = np.zeros(self.delay, dtype=int)\n",
    "        input_seq = np.concatenate((sequence, zeros))\n",
    "        target_seq = np.concatenate((zeros, sequence))\n",
    "        return torch.LongTensor(input_seq), torch.LongTensor(target_seq)\n",
    "\n",
    "class GyroCopyModel(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(num_classes + 1, hidden_dim)\n",
    "        self.cell = PureRotationCell(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 4, num_classes + 1)\n",
    "    def forward(self, x):\n",
    "        b, total_len = x.size()\n",
    "        emb = self.embedding(x)\n",
    "        h = torch.zeros(b, self.hidden_dim, 4).to(x.device)\n",
    "        h[:, :, 0] = 1.0\n",
    "        outputs = []\n",
    "        for t in range(total_len):\n",
    "            x_t = emb[:, t, :].unsqueeze(1)\n",
    "            h = self.cell(x_t, h)\n",
    "            h_flat = h.view(b, -1)\n",
    "            out_t = self.fc_out(h_flat)\n",
    "            outputs.append(out_t.unsqueeze(1))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Training\n",
    "copy_model = GyroCopyModel(8, 16).to(device)\n",
    "copy_optimizer = optim.Adam(copy_model.parameters(), lr=0.002)\n",
    "copy_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "copy_train_ds = CopyTaskDataset(seq_len=10, delay=20, size=3000, num_classes=8)\n",
    "copy_train_loader = DataLoader(copy_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "copy_model.train()\n",
    "for epoch in range(1, 60):\n",
    "    total_loss = 0; correct = 0; total = 0\n",
    "    for data, target in copy_train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        copy_optimizer.zero_grad()\n",
    "        output = copy_model(data)\n",
    "        output_flat = output.view(-1, 9)\n",
    "        target_flat = target.view(-1)\n",
    "        loss = copy_criterion(output_flat, target_flat)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(copy_model.parameters(), 1.0)\n",
    "        copy_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=-1)\n",
    "        valid_pred = pred[:, -10:]\n",
    "        valid_target = target[:, -10:]\n",
    "        correct += (valid_pred == valid_target).sum().item()\n",
    "        total += valid_target.numel()\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Copy Epoch {epoch} Loss: {total_loss/len(copy_train_loader):.4f} | Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92843fe7",
   "metadata": {},
   "source": [
    "### XOR Problem: Logical Parity Detection\n",
    "\n",
    "Detects if the number of 1s in a sequence is odd, using a counter-like mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1746ba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Epoch 1 Acc: 51.13%\n",
      "XOR Epoch 2 Acc: 50.47%\n",
      "XOR Epoch 3 Acc: 50.40%\n",
      "XOR Epoch 4 Acc: 64.90%\n",
      "XOR Epoch 5 Acc: 99.07%\n"
     ]
    }
   ],
   "source": [
    "class TemporalXORDataset(Dataset):\n",
    "    def __init__(self, seq_len=50, size=3000):\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self, idx):\n",
    "        seq = np.random.randint(0, 2, size=self.seq_len)\n",
    "        target = seq.sum() % 2  # 1 if odd number of 1s\n",
    "        return torch.tensor(seq, dtype=torch.float).unsqueeze(-1), torch.tensor(target, dtype=torch.float)\n",
    "\n",
    "class SmartCounterGyro(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.ih = nn.Linear(1, hidden_dim * 4)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim * 4, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1))\n",
    "    def forward(self, x):\n",
    "        b, seq, _ = x.size()\n",
    "        h = torch.zeros(b, self.hidden_dim, 4).to(x.device)\n",
    "        h[:, :, 0] = 1.0\n",
    "        for t in range(seq):\n",
    "            x_t = x[:, t, :]\n",
    "            rot_command = self.ih(x_t).view(b, -1, 4)\n",
    "            rot_command = rot_command / (quaternion_norm(rot_command) + 1e-8)\n",
    "            h = quaternion_mul(rot_command, h)\n",
    "        flat = h.view(b, -1)\n",
    "        return self.fc(flat).squeeze()\n",
    "\n",
    "# Training\n",
    "xor_model = SmartCounterGyro(16).to(device)\n",
    "xor_optimizer = optim.Adam(xor_model.parameters(), lr=0.005)\n",
    "xor_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "xor_train_ds = TemporalXORDataset(seq_len=50, size=3000)\n",
    "xor_train_loader = DataLoader(xor_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "xor_model.train()\n",
    "for epoch in range(1, 6):\n",
    "    correct = 0; total = 0\n",
    "    for data, target in xor_train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        xor_optimizer.zero_grad()\n",
    "        output = xor_model(data)\n",
    "        loss = xor_criterion(output, target)\n",
    "        loss.backward()\n",
    "        xor_optimizer.step()\n",
    "        preds = (torch.sigmoid(output) > 0.5).float()\n",
    "        correct += (preds == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"XOR Epoch {epoch} Acc: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c262aa",
   "metadata": {},
   "source": [
    "### Dyck Language: Bracket Parsing\n",
    "\n",
    "Classifies valid nested brackets, requiring stack-like memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a369d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DyckDataset(Dataset):\n",
    "    def __init__(self, size=3000, max_depth=10):\n",
    "        self.size = size\n",
    "        self.max_depth = max_depth\n",
    "        # 0:PAD, 1:(, 2:), 3:[, 4:]\n",
    "        self.vocab = {'(': 1, ')': 2, '[': 3, ']': 4, 'PAD': 0}\n",
    "        \n",
    "    def generate_dyck(self):\n",
    "        s = []\n",
    "        stack = []\n",
    "        length = random.randint(2, self.max_depth * 2)\n",
    "        if length % 2 != 0: length += 1\n",
    "        \n",
    "        while len(s) < length:\n",
    "            if len(stack) == 0 or (len(s) + len(stack) < length and random.random() > 0.5):\n",
    "                char = random.choice(['(', '['])\n",
    "                s.append(char)\n",
    "                stack.append(char)\n",
    "            else:\n",
    "                opener = stack.pop()\n",
    "                closer = ')' if opener == '(' else ']'\n",
    "                s.append(closer)\n",
    "        return s\n",
    "\n",
    "    def generate_invalid(self):\n",
    "        s = self.generate_dyck()\n",
    "        if len(s) < 2: return ['(', ')'] # Fallback\n",
    "        if random.random() > 0.5:\n",
    "            i, j = random.sample(range(len(s)), 2)\n",
    "            s[i], s[j] = s[j], s[i]\n",
    "        else:\n",
    "            i = random.randint(0, len(s)-1)\n",
    "            s[i] = random.choice(['(', ')', '[', ']'])\n",
    "        return s\n",
    "\n",
    "    def __len__(self): return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        is_valid = random.random() > 0.5\n",
    "        if is_valid:\n",
    "            seq_chars = self.generate_dyck()\n",
    "            label = 1.0\n",
    "        else:\n",
    "            seq_chars = self.generate_invalid()\n",
    "            label = 0.0 # We map this to -1 for regression or kept as 0 for sigmoid\n",
    "            \n",
    "        seq = [self.vocab[c] for c in seq_chars]\n",
    "        pad_len = self.max_depth * 2 + 2\n",
    "        if len(seq) < pad_len:\n",
    "            seq += [0] * (pad_len - len(seq))\n",
    "            \n",
    "        return torch.LongTensor(seq), torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "425c4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyck Epoch 1 Acc: 50.50%\n",
      "Dyck Epoch 2 Acc: 54.10%\n",
      "Dyck Epoch 2 Acc: 54.10%\n",
      "Dyck Epoch 3 Acc: 55.60%\n",
      "Dyck Epoch 3 Acc: 55.60%\n",
      "Dyck Epoch 4 Acc: 61.60%\n",
      "Dyck Epoch 4 Acc: 61.60%\n",
      "Dyck Epoch 5 Acc: 62.60%\n",
      "Dyck Epoch 5 Acc: 62.60%\n",
      "Dyck Epoch 6 Acc: 62.90%\n",
      "Dyck Epoch 6 Acc: 62.90%\n",
      "Dyck Epoch 7 Acc: 68.15%\n",
      "Dyck Epoch 7 Acc: 68.15%\n",
      "Dyck Epoch 8 Acc: 70.85%\n",
      "Dyck Epoch 8 Acc: 70.85%\n",
      "Dyck Epoch 9 Acc: 75.60%\n",
      "Dyck Epoch 9 Acc: 75.60%\n",
      "Dyck Epoch 10 Acc: 74.10%\n",
      "Dyck Epoch 10 Acc: 74.10%\n",
      "Dyck Epoch 11 Acc: 75.75%\n",
      "Dyck Epoch 11 Acc: 75.75%\n",
      "Dyck Epoch 12 Acc: 76.90%\n",
      "Dyck Epoch 12 Acc: 76.90%\n",
      "Dyck Epoch 13 Acc: 75.15%\n",
      "Dyck Epoch 13 Acc: 75.15%\n",
      "Dyck Epoch 14 Acc: 76.80%\n",
      "Dyck Epoch 14 Acc: 76.80%\n",
      "Dyck Epoch 15 Acc: 78.10%\n",
      "Dyck Epoch 15 Acc: 78.10%\n",
      "Dyck Epoch 16 Acc: 74.70%\n",
      "Dyck Epoch 16 Acc: 74.70%\n",
      "Dyck Epoch 17 Acc: 75.65%\n",
      "Dyck Epoch 17 Acc: 75.65%\n",
      "Dyck Epoch 18 Acc: 76.50%\n",
      "Dyck Epoch 18 Acc: 76.50%\n",
      "Dyck Epoch 19 Acc: 78.20%\n",
      "Dyck Epoch 19 Acc: 78.20%\n",
      "Dyck Epoch 20 Acc: 77.70%\n",
      "Dyck Epoch 20 Acc: 77.70%\n",
      "Dyck Epoch 21 Acc: 78.45%\n",
      "Dyck Epoch 21 Acc: 78.45%\n",
      "Dyck Epoch 22 Acc: 78.35%\n",
      "Dyck Epoch 22 Acc: 78.35%\n",
      "Dyck Epoch 23 Acc: 77.75%\n",
      "Dyck Epoch 23 Acc: 77.75%\n",
      "Dyck Epoch 24 Acc: 77.15%\n",
      "Dyck Epoch 24 Acc: 77.15%\n",
      "Dyck Epoch 25 Acc: 79.15%\n",
      "Dyck Epoch 25 Acc: 79.15%\n",
      "Dyck Epoch 26 Acc: 79.60%\n",
      "Dyck Epoch 26 Acc: 79.60%\n",
      "Dyck Epoch 27 Acc: 80.00%\n",
      "Dyck Epoch 27 Acc: 80.00%\n",
      "Dyck Epoch 28 Acc: 78.50%\n",
      "Dyck Epoch 28 Acc: 78.50%\n",
      "Dyck Epoch 29 Acc: 79.45%\n",
      "Dyck Epoch 29 Acc: 79.45%\n",
      "Dyck Epoch 30 Acc: 79.85%\n",
      "Dyck Epoch 30 Acc: 79.85%\n",
      "Dyck Epoch 31 Acc: 78.40%\n",
      "Dyck Epoch 31 Acc: 78.40%\n",
      "Dyck Epoch 32 Acc: 78.55%\n",
      "Dyck Epoch 32 Acc: 78.55%\n",
      "Dyck Epoch 33 Acc: 79.30%\n",
      "Dyck Epoch 33 Acc: 79.30%\n",
      "Dyck Epoch 34 Acc: 79.65%\n",
      "Dyck Epoch 34 Acc: 79.65%\n",
      "Dyck Epoch 35 Acc: 79.15%\n",
      "Dyck Epoch 35 Acc: 79.15%\n",
      "Dyck Epoch 36 Acc: 81.75%\n",
      "Dyck Epoch 36 Acc: 81.75%\n",
      "Dyck Epoch 37 Acc: 79.85%\n",
      "Dyck Epoch 37 Acc: 79.85%\n",
      "Dyck Epoch 38 Acc: 80.35%\n",
      "Dyck Epoch 38 Acc: 80.35%\n",
      "Dyck Epoch 39 Acc: 80.00%\n",
      "Dyck Epoch 39 Acc: 80.00%\n",
      "Dyck Epoch 40 Acc: 80.35%\n",
      "Dyck Epoch 40 Acc: 80.35%\n",
      "Dyck Epoch 41 Acc: 81.15%\n",
      "Dyck Epoch 41 Acc: 81.15%\n",
      "Dyck Epoch 42 Acc: 79.45%\n",
      "Dyck Epoch 42 Acc: 79.45%\n",
      "Dyck Epoch 43 Acc: 80.20%\n",
      "Dyck Epoch 43 Acc: 80.20%\n",
      "Dyck Epoch 44 Acc: 81.70%\n",
      "Dyck Epoch 44 Acc: 81.70%\n",
      "Dyck Epoch 45 Acc: 81.45%\n",
      "Dyck Epoch 45 Acc: 81.45%\n",
      "Dyck Epoch 46 Acc: 78.60%\n",
      "Dyck Epoch 46 Acc: 78.60%\n",
      "Dyck Epoch 47 Acc: 81.05%\n",
      "Dyck Epoch 47 Acc: 81.05%\n",
      "Dyck Epoch 48 Acc: 82.35%\n",
      "Dyck Epoch 48 Acc: 82.35%\n",
      "Dyck Epoch 49 Acc: 80.85%\n",
      "Dyck Epoch 49 Acc: 80.85%\n",
      "Dyck Epoch 50 Acc: 82.45%\n",
      "Dyck Epoch 50 Acc: 82.45%\n",
      "Dyck Epoch 51 Acc: 81.70%\n",
      "Dyck Epoch 51 Acc: 81.70%\n",
      "Dyck Epoch 52 Acc: 80.60%\n",
      "Dyck Epoch 52 Acc: 80.60%\n",
      "Dyck Epoch 53 Acc: 81.25%\n",
      "Dyck Epoch 53 Acc: 81.25%\n",
      "Dyck Epoch 54 Acc: 81.90%\n",
      "Dyck Epoch 54 Acc: 81.90%\n",
      "Dyck Epoch 55 Acc: 81.55%\n",
      "Dyck Epoch 55 Acc: 81.55%\n",
      "Dyck Epoch 56 Acc: 82.50%\n",
      "Dyck Epoch 56 Acc: 82.50%\n",
      "Dyck Epoch 57 Acc: 81.25%\n",
      "Dyck Epoch 57 Acc: 81.25%\n",
      "Dyck Epoch 58 Acc: 82.00%\n",
      "Dyck Epoch 58 Acc: 82.00%\n",
      "Dyck Epoch 59 Acc: 81.80%\n",
      "Dyck Epoch 59 Acc: 81.80%\n",
      "Dyck Epoch 60 Acc: 80.70%\n",
      "Dyck Epoch 60 Acc: 80.70%\n"
     ]
    }
   ],
   "source": [
    "class UnifiedDyckWrapper(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(5, 8)  # Updated for 5 classes: 0:PAD, 1:(, 2:), 3:[, 4:]\n",
    "        self.cell = PureRotationCell(8, hidden_dim)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim * 4, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, 1))\n",
    "    def forward(self, x):\n",
    "        b, seq = x.size()\n",
    "        emb = self.emb(x).view(b, seq, -1)\n",
    "        h = torch.zeros(b, self.cell.hidden_size, 4).to(x.device)\n",
    "        h[:, :, 0] = 1.0\n",
    "        for t in range(seq):\n",
    "            x_t = emb[:, t, :].unsqueeze(1)\n",
    "            h = self.cell(x_t, h)\n",
    "        flat = h.view(b, -1)\n",
    "        return torch.sigmoid(self.fc(flat)).squeeze()\n",
    "\n",
    "# Training\n",
    "dyck_model = UnifiedDyckWrapper(16).to(device)\n",
    "dyck_optimizer = optim.Adam(dyck_model.parameters(), lr=0.005)\n",
    "dyck_criterion = nn.BCELoss()\n",
    "\n",
    "dyck_train_ds = DyckDataset(size=2000, max_depth=10)\n",
    "dyck_train_loader = DataLoader(dyck_train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "dyck_model.train()\n",
    "for epoch in range(1, 61):\n",
    "    correct = 0; total = 0\n",
    "    for data, target in dyck_train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        dyck_optimizer.zero_grad()\n",
    "        output = dyck_model(data)\n",
    "        loss = dyck_criterion(output, target)\n",
    "        loss.backward()\n",
    "        dyck_optimizer.step()\n",
    "        preds = (output > 0.5).float()\n",
    "        correct += (preds == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"Dyck Epoch {epoch} Acc: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f059573",
   "metadata": {},
   "source": [
    "### HAR: Human Activity Recognition\n",
    "\n",
    "Combines CNN for feature extraction with Tandem-Q for sequential processing of sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67c1b8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAR Epoch 1 Acc: 16.40%\n",
      "HAR Epoch 2 Acc: 17.00%\n",
      "HAR Epoch 2 Acc: 17.00%\n",
      "HAR Epoch 3 Acc: 16.20%\n",
      "HAR Epoch 3 Acc: 16.20%\n",
      "HAR Epoch 4 Acc: 16.80%\n",
      "HAR Epoch 4 Acc: 16.80%\n",
      "HAR Epoch 5 Acc: 17.90%\n",
      "HAR Epoch 5 Acc: 17.90%\n",
      "HAR Epoch 6 Acc: 15.50%\n",
      "HAR Epoch 6 Acc: 15.50%\n",
      "HAR Epoch 7 Acc: 14.50%\n",
      "HAR Epoch 7 Acc: 14.50%\n",
      "HAR Epoch 8 Acc: 17.80%\n",
      "HAR Epoch 8 Acc: 17.80%\n",
      "HAR Epoch 9 Acc: 18.10%\n",
      "HAR Epoch 9 Acc: 18.10%\n",
      "HAR Epoch 10 Acc: 16.00%\n",
      "HAR Epoch 10 Acc: 16.00%\n",
      "HAR Epoch 11 Acc: 16.30%\n",
      "HAR Epoch 11 Acc: 16.30%\n",
      "HAR Epoch 12 Acc: 16.30%\n",
      "HAR Epoch 12 Acc: 16.30%\n",
      "HAR Epoch 13 Acc: 14.40%\n",
      "HAR Epoch 13 Acc: 14.40%\n",
      "HAR Epoch 14 Acc: 16.50%\n",
      "HAR Epoch 14 Acc: 16.50%\n",
      "HAR Epoch 15 Acc: 16.40%\n",
      "HAR Epoch 15 Acc: 16.40%\n",
      "HAR Epoch 16 Acc: 16.70%\n",
      "HAR Epoch 16 Acc: 16.70%\n",
      "HAR Epoch 17 Acc: 16.80%\n",
      "HAR Epoch 17 Acc: 16.80%\n",
      "HAR Epoch 18 Acc: 16.80%\n",
      "HAR Epoch 18 Acc: 16.80%\n",
      "HAR Epoch 19 Acc: 16.80%\n",
      "HAR Epoch 19 Acc: 16.80%\n",
      "HAR Epoch 20 Acc: 17.20%\n",
      "HAR Epoch 20 Acc: 17.20%\n",
      "HAR Epoch 21 Acc: 17.00%\n",
      "HAR Epoch 21 Acc: 17.00%\n",
      "HAR Epoch 22 Acc: 15.30%\n",
      "HAR Epoch 22 Acc: 15.30%\n",
      "HAR Epoch 23 Acc: 17.60%\n",
      "HAR Epoch 23 Acc: 17.60%\n",
      "HAR Epoch 24 Acc: 16.90%\n",
      "HAR Epoch 24 Acc: 16.90%\n",
      "HAR Epoch 25 Acc: 19.90%\n",
      "HAR Epoch 25 Acc: 19.90%\n",
      "HAR Epoch 26 Acc: 18.10%\n",
      "HAR Epoch 26 Acc: 18.10%\n",
      "HAR Epoch 27 Acc: 16.20%\n",
      "HAR Epoch 27 Acc: 16.20%\n",
      "HAR Epoch 28 Acc: 18.70%\n",
      "HAR Epoch 28 Acc: 18.70%\n",
      "HAR Epoch 29 Acc: 17.00%\n",
      "HAR Epoch 29 Acc: 17.00%\n",
      "HAR Epoch 30 Acc: 17.00%\n",
      "HAR Epoch 30 Acc: 17.00%\n",
      "HAR Epoch 31 Acc: 15.10%\n",
      "HAR Epoch 31 Acc: 15.10%\n",
      "HAR Epoch 32 Acc: 15.30%\n",
      "HAR Epoch 32 Acc: 15.30%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m data, target = data.to(device), target.to(device)\n\u001b[32m     43\u001b[39m har_optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m output = \u001b[43mhar_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m loss = har_criterion(output, target)\n\u001b[32m     46\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mHybridGyroModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     25\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool(x)\n\u001b[32m     26\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(h.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaweh\\.conda\\envs\\gyro_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mTandemQCell.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     32\u001b[39m rot_i, rot_j, rot_k = rot_raw[:, :, \u001b[32m1\u001b[39m], rot_raw[:, :, \u001b[32m2\u001b[39m], rot_raw[:, :, \u001b[32m3\u001b[39m]\n\u001b[32m     34\u001b[39m scale = torch.sigmoid(raw[:, :, \u001b[32m4\u001b[39m] + \u001b[32m3.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m norm = quaternion_norm(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrot_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrot_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrot_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrot_k\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     36\u001b[39m norm_squeezed = norm.squeeze(-\u001b[32m1\u001b[39m)\n\u001b[32m     38\u001b[39m new_r, new_i, new_j, new_k = quaternion_mul(\n\u001b[32m     39\u001b[39m     torch.stack((rot_r/norm_squeezed, rot_i/norm_squeezed, rot_j/norm_squeezed, rot_k/norm_squeezed), dim=-\u001b[32m1\u001b[39m),\n\u001b[32m     40\u001b[39m     torch.stack((h_r, h_i, h_j, h_k), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     41\u001b[39m ).unbind(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "def load_uci_har_production():\n",
    "    if not os.path.exists(\"UCI HAR Dataset\"):\n",
    "        print(\"Downloading UCI HAR Dataset (60MB)...\")\n",
    "        url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, \"uci_har.zip\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            return None, None, None, None\n",
    "            \n",
    "        print(\"Extracting...\")\n",
    "        with zipfile.ZipFile(\"uci_har.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "    \n",
    "    print(\"Loading Data...\")\n",
    "    base = \"UCI HAR Dataset\"\n",
    "    \n",
    "    def load_file(subset, filename):\n",
    "        return np.loadtxt(f\"{base}/{subset}/Inertial Signals/{filename}_{subset}.txt\")\n",
    "    def load_y(subset):\n",
    "        return np.loadtxt(f\"{base}/{subset}/y_{subset}.txt\")\n",
    "    \n",
    "    # Load 6 channels\n",
    "    signals = [\"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\"]\n",
    "    \n",
    "    X_train = np.dstack([load_file(\"train\", s) for s in signals])\n",
    "    X_test = np.dstack([load_file(\"test\", s) for s in signals])\n",
    "    y_train = load_y(\"train\") - 1; y_test = load_y(\"test\") - 1\n",
    "    \n",
    "    return torch.FloatTensor(X_train), torch.LongTensor(y_train), torch.FloatTensor(X_test), torch.LongTensor(y_test)\n",
    "\n",
    "class TandemQCell(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super(TandemQCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.ih = nn.Linear(input_size, hidden_size * 5)\n",
    "        self.hh = nn.Linear(hidden_size * 4, hidden_size * 5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.ih.weight.data.normal_(0, 0.01); self.ih.bias.fill_(0.0)\n",
    "            self.hh.weight.fill_(0.0); self.hh.bias.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq, _ = x.size()\n",
    "        gates_in = self.ih(x) \n",
    "        \n",
    "        h_r = torch.ones(b, self.hidden_size, device=x.device)\n",
    "        h_i = torch.zeros(b, self.hidden_size, device=x.device)\n",
    "        h_j = torch.zeros(b, self.hidden_size, device=x.device)\n",
    "        h_k = torch.zeros(b, self.hidden_size, device=x.device)\n",
    "        \n",
    "        for t in range(seq):\n",
    "            g_in = gates_in[:, t, :]\n",
    "            h_cat = torch.cat((h_r, h_i, h_j, h_k), dim=1)\n",
    "            g_hh = self.hh(h_cat)\n",
    "            \n",
    "            raw = g_in + g_hh\n",
    "            raw = raw.view(b, self.hidden_size, 5)\n",
    "            \n",
    "            # 1. Rotation with Tanh Damping (The Shock Absorber)\n",
    "            rot_raw = raw[:, :, :4]\n",
    "            rot_damped = torch.tanh(rot_raw) # DAMPING\n",
    "            \n",
    "            rot_r = rot_damped[:, :, 0] + 1.0\n",
    "            rot_i = rot_damped[:, :, 1]; rot_j = rot_damped[:, :, 2]; rot_k = rot_damped[:, :, 3]\n",
    "            \n",
    "            scale_raw = raw[:, :, 4]\n",
    "            \n",
    "            norm = quaternion_norm(rot_r, rot_i, rot_j, rot_k)\n",
    "            scale = torch.sigmoid(scale_raw + 1.0) # Balanced Memory\n",
    "            \n",
    "            new_r, new_i, new_j, new_k = quaternion_mul(\n",
    "                rot_r/norm, rot_i/norm, rot_j/norm, rot_k/norm, \n",
    "                h_r, h_i, h_j, h_k\n",
    "            )\n",
    "            \n",
    "            h_r = new_r * scale; h_i = new_i * scale; h_j = new_j * scale; h_k = new_k * scale\n",
    "            \n",
    "        return torch.stack((h_r, h_i, h_j, h_k), dim=1)\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE HYBRID MODEL (CNN + Gyro)\n",
    "# ==========================================\n",
    "class HybridGyroModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. FEATURE EXTRACTOR (CNN)\n",
    "        # Input: (Batch, Channels, Seq) -> (Batch, 16, Seq)\n",
    "        # Kernel 5: Smooths out 5 steps of noise at a time.\n",
    "        self.conv = nn.Conv1d(input_dim, 16, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2) # Downsample: 128 steps -> 64 steps\n",
    "        \n",
    "        # 2. SEQUENTIAL MEMORY (Tandem-Q)\n",
    "        # Input to RNN is now 16 (from CNN)\n",
    "        self.cell = torch.jit.script(TandemQCell(16, hidden_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 4, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, Seq, Channels) -> (B, Channels, Seq) for Conv\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # CNN Pass\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x) # (B, 16, 64)\n",
    "        \n",
    "        # Back to Sequence First for RNN: (B, 64, 16)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # RNN Pass\n",
    "        h = self.cell(x) # (B, 4, H)\n",
    "        \n",
    "        return self.fc(h.view(x.size(0), -1))\n",
    "\n",
    "# Training\n",
    "X_train, y_train, X_test, y_test = load_uci_har_production()\n",
    "if X_train is None:\n",
    "    print(\"Failed to load UCI HAR. Using synthetic data.\")\n",
    "    har_model = HybridGyroModel(6, 12, 6).to(device)\n",
    "    har_optimizer = optim.Adam(har_model.parameters(), lr=0.003)\n",
    "    har_criterion = nn.CrossEntropyLoss()\n",
    "    har_train_ds = HardDataset(seq_len=128, size=1000, channels=6, classes=6)\n",
    "    har_train_loader = DataLoader(har_train_ds, batch_size=64, shuffle=True)\n",
    "    har_model.train()\n",
    "    for epoch in range(1, 6):\n",
    "        correct = 0; total = 0\n",
    "        for data, target in har_train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            har_optimizer.zero_grad()\n",
    "            output = har_model(data)\n",
    "            loss = har_criterion(output, target)\n",
    "            loss.backward()\n",
    "            har_optimizer.step()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "        acc = 100. * correct / total\n",
    "        print(f\"HAR Epoch {epoch} Acc: {acc:.2f}%\")\n",
    "else:\n",
    "    print(f\"Loaded UCI HAR: Train {X_train.shape}, Test {X_test.shape}\")\n",
    "    train_ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    test_ds = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    har_model = HybridGyroModel(6, 12, 6).to(device)\n",
    "    har_optimizer = optim.Adam(har_model.parameters(), lr=0.003)\n",
    "    har_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    har_model.train()\n",
    "    for epoch in range(1, 61):\n",
    "        correct = 0; total = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            har_optimizer.zero_grad()\n",
    "            output = har_model(data)\n",
    "            loss = har_criterion(output, target)\n",
    "            loss.backward()\n",
    "            har_optimizer.step()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "        acc = 100. * correct / total\n",
    "        print(f\"HAR Epoch {epoch} Train Acc: {acc:.2f}%\")\n",
    "    \n",
    "    har_model.eval()\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = har_model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    test_acc = 100. * correct / total\n",
    "    print(f\"HAR Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nAll experiments demonstrate Tandem-Q's adaptability across diverse sequence tasks!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada26bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb8f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gyro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
